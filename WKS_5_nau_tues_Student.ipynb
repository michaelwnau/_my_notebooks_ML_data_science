{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaelwnau/ai_academy_notebooks/blob/main/WKS_5_nau_tues_Student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMyRNp6GV88M"
      },
      "source": [
        "# **Workshop 5 (Student)**\n",
        "\n",
        "In this workshop, you'll looking at evaluation metrics and hyperparameter turning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouC4Kc1OV88O"
      },
      "source": [
        "# 0) Loading Data and Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAvk9qLsV88O"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.metrics\n",
        "# we're using the Diabetes dataset from sklearn.datasets\n",
        "from sklearn import datasets\n",
        "# Remember you have to run this cell block before continuing!\n",
        "\n",
        "# set a seed for reproducibility\n",
        "random_seed = 25\n",
        "np.random.seed(random_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8e2Y_VvV88P"
      },
      "source": [
        "# 1) Evaluation Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrobaNhlV88P"
      },
      "source": [
        "## 1.1) Meet the Metrics (Follow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a4pHsZXV88P"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "# This is a dummy dataset that contains 500 positive and 500 negative samples\n",
        "X,Y = make_classification(n_samples=1000,n_features=4,flip_y=0,random_state=random_seed)\n",
        "\n",
        "test_data_fraction = 0.2\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_data_fraction,  random_state=random_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96uFQVfxV88Q"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "Y_test_predicted = DecisionTreeClassifier(criterion = \"gini\", random_state=random_seed).fit(X=X_train, y=Y_train).predict(X_test)\n",
        "#sum(Y)/len(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKRKmzUrV88Q"
      },
      "outputs": [],
      "source": [
        "# BEGIN SOLUTION\n",
        "print(f'Accuracy: {sklearn.metrics.accuracy_score(Y_test, Y_test_predicted)}')\n",
        "print(f'Precision Macro: {sklearn.metrics.precision_score(Y_test, Y_test_predicted, average=\"macro\")}')\n",
        "print(f'Recall Macro: {sklearn.metrics.recall_score(Y_test, Y_test_predicted, average=\"macro\")}')\n",
        "print(f'F1 Macro: { sklearn.metrics.f1_score(Y_test, Y_test_predicted, average=\"macro\") }')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kEUQjFRV88Q"
      },
      "outputs": [],
      "source": [
        "# Since the datset is balanced in term of class distribution, all of the micro scores are the same as the accuracy\n",
        "print(f'Precision Micro: {sklearn.metrics.precision_score(Y_test, Y_test_predicted, average=\"micro\")}')\n",
        "print(f'Recall Micro: {sklearn.metrics.recall_score(Y_test, Y_test_predicted, average=\"micro\")}')\n",
        "print(f'F1 Micro: { sklearn.metrics.f1_score(Y_test, Y_test_predicted, average=\"micro\") }')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS-CzfcSV88Q"
      },
      "source": [
        "Sklearn also has a [built in function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) that will give a handy summary of all the popular classification metrics. You can use this for the later questions.\n",
        "\n",
        "The first few values on the first column (before accuracy, macro avg, etc.) are the class values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmewc4SxV88R"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(Y_test,Y_test_predicted,digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "np706yHHV88R"
      },
      "outputs": [],
      "source": [
        "# K-Nearest Neighbor Classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "Y_test_predicted = KNeighborsClassifier(n_neighbors=3).fit(X=X_train, y=Y_train).predict(X_test)\n",
        "print(\"KNN Classifer\")\n",
        "print(classification_report(Y_test,Y_test_predicted,digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZ7ICEijV88R"
      },
      "outputs": [],
      "source": [
        "# AdaBoost Classifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "Y_test_predicted = AdaBoostClassifier(n_estimators=100, random_state=random_seed).fit(X=X_train, y=Y_train).predict(X_test)\n",
        "print(\"Adaboost Classifier\")\n",
        "print(classification_report(Y_test,Y_test_predicted,digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCBf7hSwV88R"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Dummy Classifier (Picks the majority class. Every time.)\n",
        "from sklearn.dummy import DummyClassifier\n",
        "Y_test_predicted = DummyClassifier(strategy=\"most_frequent\", random_state=random_seed).fit(X=X_train, y=Y_train).predict(X_test)\n",
        "print(\"Dummy Classifier\")\n",
        "print(classification_report(Y_test,Y_test_predicted,digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQ8-ARaUV88R"
      },
      "source": [
        "## 1.2) Imbalanced data (Group)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZF3HLVAV88R"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "# Read the breast cancer dataset and translate to pandas dataframe\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "bc_sk = datasets.load_breast_cancer()\n",
        "\n",
        "# Make sure data is in the same range\n",
        "bc_sk.data = MinMaxScaler().fit_transform(bc_sk.data)\n",
        "\n",
        "# Note that the \"target\" attribute is species, represented as an integer\n",
        "bc_data = pd.DataFrame(data= np.c_[bc_sk['data'], bc_sk['target']],columns= list(bc_sk['feature_names'])+['target'])\n",
        "bc_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6sclTX8V88R"
      },
      "outputs": [],
      "source": [
        "test_data_fraction = 0.2\n",
        "bc_features = bc_data.iloc[:,0:-1]\n",
        "bc_labels = bc_data[\"target\"]\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(bc_features, bc_labels, test_size=test_data_fraction,  random_state=random_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev8zpBRpV88S"
      },
      "source": [
        "Let's take a look at the ratio of class values in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0oLtUW7V88S"
      },
      "outputs": [],
      "source": [
        "bc_data[\"target\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyo5ABUPV88S"
      },
      "source": [
        "As we can see, it's around a 60/40 split. What effect do you think this will have on the various evaluation metrics?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxC9YxI7V88S"
      },
      "source": [
        "**Discuss Here**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y5tW1ipV88S"
      },
      "source": [
        "Now run the evaluation metrics as like above for Decision Trees, KNN, Adaboost, and the Dummy Classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcRypdDZV88S"
      },
      "outputs": [],
      "source": [
        "# Decision Tree\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLSNFDAhV88S"
      },
      "outputs": [],
      "source": [
        "# K-Nearest Neighbor Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZg2tIVCV88S"
      },
      "outputs": [],
      "source": [
        "# AdaBoost Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZTqzISWV88S"
      },
      "outputs": [],
      "source": [
        "# Dummy Classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilzPvI28V88S"
      },
      "source": [
        "In terms of evaluation metrics, how did each model perform? Discuss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFIxqYT0V88S"
      },
      "source": [
        "**Discuss**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLg9RIELV88S"
      },
      "source": [
        "## 1.3) Multiclass Data (Group)\n",
        "\n",
        "Now, we'll be looking at the wine dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3oJ1E_MV88S"
      },
      "outputs": [],
      "source": [
        "# Read the iris dataset and translate to pandas dataframe\n",
        "wine_sk = datasets.load_wine()\n",
        "# Note that the \"target\" attribute is species, represented as an integer\n",
        "wine_data = pd.DataFrame(data= np.c_[wine_sk['data'], wine_sk['target']],columns= wine_sk['feature_names'] + ['target'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bffccFeUV88S"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# The fraction of data that will be test data\n",
        "test_data_fraction = 0.1\n",
        "\n",
        "wine_features = wine_data.iloc[:,0:-1]\n",
        "wine_labels = wine_data[\"target\"]\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(wine_features, wine_labels, test_size=test_data_fraction,  random_state=random_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGP3z4FmV88T"
      },
      "source": [
        "Let's check the distribution of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0W8o0_geV88T"
      },
      "outputs": [],
      "source": [
        "wine_data[\"target\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fme97D9FV88T"
      },
      "source": [
        "The [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) is useful for getting a broad overview of how your classifier handled certain classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZNJftOJV88T"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Now create a confusion matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pJXbgb1V88T"
      },
      "source": [
        "Now run the evaluation metrics as like above for Decision Trees, KNN, Adaboost, and the Dummy Classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qL0o9tD9V88T"
      },
      "outputs": [],
      "source": [
        "# Decision Tree\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbCHazCfV88T"
      },
      "outputs": [],
      "source": [
        "# K-Nearest Neighbor Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrnB18CpV88T"
      },
      "outputs": [],
      "source": [
        "# AdaBoost Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrVKF7fSV88T"
      },
      "outputs": [],
      "source": [
        "# Dummy Classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpWIZI0wV88T"
      },
      "source": [
        "In terms of evaluation metrics, how did each model perform? Discuss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbCt7DQxV88T"
      },
      "source": [
        "Discuss here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lptFcf5V88T"
      },
      "source": [
        "# 2) Cross Validation and Hyperparmeter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7AJRXArV88U"
      },
      "source": [
        "## 2.1) Basic Cross Validation (Follow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtENmSgeV88U"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKDxa9rUV88U"
      },
      "outputs": [],
      "source": [
        "# Initialize a k-fold splitter\n",
        "kf = KFold(n_splits=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_sb6sdoV88U"
      },
      "outputs": [],
      "source": [
        "# Kf.split() allows you to iterate though the different folds\n",
        "# \"train_index\" are the indecies of the training data in that fold\n",
        "# \"test_index\" are the indicies of the testing data in that fold\n",
        "print(len(X_train))\n",
        "for train_index, test_index in kf.split(X_train):\n",
        "    print(\"Train: \", train_index)\n",
        "    print(\"Test: \", test_index)\n",
        "    print(\"----\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6AcNBTTV88U"
      },
      "source": [
        "## 2.2) Hyperparameter Tuning with CV (Group)\n",
        "\n",
        "We did some very basic HP Tuning last workshop. However, one of the main issues is that we did HP tuning by testing our HPs againt the test dataset. It's good practice not to touch your dataset at all until you've finished selecting your model completly. Therefore, in this exercise we'll be trying out different HPs by constructing validation sets from our training data.\n",
        "\n",
        "The dataset we'll be using for this exercise is the breast cancer dataset, which is used to tell if a certain individal might have breast cancer or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnWlka8vV88U"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "# Read the wine dataset and translate to pandas dataframe\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "bc_sk = datasets.load_breast_cancer()\n",
        "\n",
        "# Make sure data is in the same range\n",
        "bc_sk.data = MinMaxScaler().fit_transform(bc_sk.data)\n",
        "\n",
        "# Note that the \"target\" attribute is species, represented as an integer\n",
        "bc_data = pd.DataFrame(data= np.c_[bc_sk['data'], bc_sk['target']],columns= list(bc_sk['feature_names'])+['target'])\n",
        "bc_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXT5r-zCV88U"
      },
      "outputs": [],
      "source": [
        "# Formatting our data\n",
        "test_data_fraction = 0.2\n",
        "bc_features = bc_data.iloc[:,0:-1]\n",
        "bc_labels = bc_data[\"target\"]\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(bc_features, bc_labels, test_size=test_data_fraction,  random_state=random_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQLqcKgIV88U"
      },
      "outputs": [],
      "source": [
        "def k_fold_accuracy(k, model, X_data, Y_data):\n",
        "    \n",
        "    # Init k-fold splitter\n",
        "    kf = KFold(n_splits=k)\n",
        "    scores = []\n",
        "    \n",
        "    #use kf.split to split the train data into train and validation data\n",
        "    #iterate through all possible folds and fit the folded training data to the model\n",
        "    #use the validation data to predict on the model\n",
        "    #compute the accuracy score and append it to scores\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7-GWcJrV88V"
      },
      "outputs": [],
      "source": [
        "# Testing K-fold\n",
        "k = 3\n",
        "model = DecisionTreeClassifier(criterion = \"gini\", random_state=random_seed)\n",
        "per_fold_acc = k_fold_accuracy(k, model, X_train, Y_train)\n",
        "print(per_fold_acc)\n",
        "np.mean(per_fold_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5MNnj2kV88V"
      },
      "source": [
        "There also exists a built in sklearn function for this, however it is import to know how to perform your own k-fold cross validation split if you want to implement a custom evaluation metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGQ1hFW-V88V"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "# We're using the trianing dataset here, but remember that CV will\n",
        "# split that data into training and validation sets for each fold\n",
        "# so we get an \"unbiased\" estimate of our test performance.\n",
        "per_fold_acc = cross_val_score(model, X_train.values, Y_train.values, cv=KFold(n_splits=k), scoring='accuracy')\n",
        "print(per_fold_acc)\n",
        "np.mean(per_fold_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGVyvgsrV88V"
      },
      "source": [
        "## 2.3 Tuning (Group)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DsaQbQnV88V"
      },
      "source": [
        "In this problem you are going to select the best hypterparameter, using *only the training dataset*. No peaking at the test dataset. To estimate how well a given hyperparameter value will do on *unseen* data, we can use Crossvalidation (within the training dataset) to evaluate our model.\n",
        "\n",
        "You should:\n",
        "1. Iterate over all ccp_alpha values\n",
        "2. Calculate the k_fold validation accuracy using the above funciton\n",
        "3. Calculate the training accuracy and the validation accuracy\n",
        "4. Plot both accuracies vs. the ccp_alpha value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cx_FGRURV88V"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# np.arange generates a list that starts at minimum, ends at maximum, and increments by step\n",
        "alpha_values = np.arange(0, 0.035, 0.002)\n",
        "\n",
        "# two lists to hold our accuracy\n",
        "k = 5\n",
        "valid_accs = []\n",
        "train_accs = []\n",
        "\n",
        "# Put your solution here!\n",
        "\n",
        "\n",
        "\n",
        "plt.plot(alpha_values, valid_accs, color='red')\n",
        "plt.plot(alpha_values, train_accs, color='blue')\n",
        "plt.xlabel(\"Post Pruning Alpha\")\n",
        "plt.ylabel(f'Average Accuracy of {k}-fold validation')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JroCSlsjV88V"
      },
      "source": [
        "The following code selects the alpha value for the best model. Then you job is to train a new model (using all of the training data), using your best hyperparameter value. Then evaluate it on the test dataset. What is the accuracy, precision, recall and F1 Score?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lkq0fGWMV88V"
      },
      "outputs": [],
      "source": [
        "# Take the alpha for the model with the best accuracy on the *validation* set!\n",
        "best_alpha = alpha_values[np.argmax(valid_accs)]\n",
        "best_alpha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m91beK5aV88V"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import plot_tree\n",
        "\n",
        "# Train your model here. You may want to print the tree using plot_tree\n",
        "# SOLUTION\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb_dbD0eV88V"
      },
      "outputs": [],
      "source": [
        "# Now evaluate your model on the test dataset - what are the evaluation metrics?\n",
        "# SOLUTION\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSJNKGMWV88V"
      },
      "source": [
        "# 3) ROC Curves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMZk5lf6V88V"
      },
      "source": [
        "Sklearn has some built in methods for [plotting ROC curves](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_roc_curve.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYaLGVlHV88V"
      },
      "source": [
        "### 3.1) Plotting ROC Curves (Group)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5zreAPAV88W"
      },
      "outputs": [],
      "source": [
        "# First make an ROC curve for the model you selected with HP tuning\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "\n",
        "gini_tree = DecisionTreeClassifier(criterion = \"gini\", random_state=random_seed, ccp_alpha=best_alpha).fit(X=X_train.values, y=Y_train.values)\n",
        "RocCurveDisplay.from_estimator(gini_tree,X_test.values,Y_test.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8-s52jXV88W"
      },
      "outputs": [],
      "source": [
        "# Now, make an ROC curve with an AdaBoostClassifier with n_estimators=100\n",
        "\n",
        "#SOLUTION\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePXBqVcHV88W"
      },
      "outputs": [],
      "source": [
        "plt.figure(0).clf()\n",
        "\n",
        "# When predicting, we have to ask for *continuous* values, not 0/1, so we use predict_proba\n",
        "# We use [:,1] to get the predictions for the positive class\n",
        "tree_predictions = gini_tree.predict_proba(X_test.values)[:,1]\n",
        "fpr, tpr, thresh = metrics.roc_curve(Y_test.values, tree_predictions)\n",
        "auc = metrics.roc_auc_score(Y_test.values, tree_predictions)\n",
        "plt.plot(fpr,tpr,label=\"Decision Tree, auc=\"+str(auc))\n",
        "\n",
        "adaboost_predictions = ada.predict_proba(X_test.values)[:,1]\n",
        "fpr, tpr, thresh = metrics.roc_curve(Y_test.values, adaboost_predictions)\n",
        "auc = metrics.roc_auc_score(Y_test.values, adaboost_predictions)\n",
        "plt.plot(fpr,tpr,label=\"Adaboost, auc=\"+str(auc))\n",
        "\n",
        "plt.legend(loc=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNlchQaAV88W"
      },
      "source": [
        "### 3.2) Intepreting ROC curves (Group)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywJ2-J7qV88W"
      },
      "source": [
        "Take a look at the above ROC curves. How are they similar? How do they differ? Is one strictly better than the other? In what situations is one better than the other? Discuss with your group."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzD15gV8V88W"
      },
      "source": [
        "**Take notes of your discussion here.**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}